# Research Powerpack MCP Server - Tool Configuration
# Single source of truth for all tool metadata, descriptions, and parameter schemas
# Version: 1.0

version: "1.0"

metadata:
  name: "research-powerpack-mcp"
  description: "Research tools for AI assistants"

tools:
  # ============================================================================
  # REDDIT TOOLS
  # ============================================================================

  - name: search_reddit
    category: reddit
    capability: search
    
    # Configurable limits
    limits:
      min_queries: 10
      max_queries: 50
      recommended_queries: 20
    
    description: |
      **ğŸ”¥ AGGRESSIVE REDDIT RESEARCH - MINIMUM 10 QUERIES REQUIRED ğŸ”¥**

      **CRITICAL:** This tool is DESIGNED for consensus analysis through MULTIPLE diverse queries.
      Using 1-3 queries = WASTING the tool's power. You MUST use 10+ queries minimum.

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“Š **QUERY BUDGET ALLOCATION** (Use ALL your query slots!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - **MINIMUM:** 10 queries (hard requirement)
      - **RECOMMENDED:** 20-30 queries (optimal consensus detection)
      - **MAXIMUM:** 50 queries (comprehensive deep research)
      
      **TOKEN ALLOCATION:** 10 results per query, all queries run in parallel
      - 10 queries = 100 total results
      - 20 queries = 200 total results
      - 50 queries = 500 total results

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ¯ **10-CATEGORY QUERY FORMULA** (Cover ALL categories!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      Each query should target a DIFFERENT angle. NO OVERLAP!
      
      1. **Direct Topic (3-5 queries):**
         - "YouTube Music Mac app"
         - "YTM desktop application"
         - "YouTube Music client macOS"
      
      2. **Recommendations (3-5 queries):**
         - "best YouTube Music client Mac"
         - "recommended YTM desktop app"
         - "top YouTube Music Mac applications"
      
      3. **Specific Tools/Projects (5-10 queries):**
         - "YTMDesktop Mac"
         - "th-ch youtube-music"
         - "steve228uk YouTube Music"
         - "youtube-music-desktop-app GitHub"
      
      4. **Comparisons (3-5 queries):**
         - "YouTube Music vs Spotify Mac desktop"
         - "YTM vs Apple Music desktop app"
         - "YouTube Music desktop vs web player"
      
      5. **Alternatives (3-5 queries):**
         - "YouTube Music Mac alternative"
         - "YTM replacement desktop"
         - "better than YouTube Music Mac"
      
      6. **Subreddit-Specific (5-10 queries):**
         - "r/YoutubeMusic desktop app"
         - "r/macapps YouTube Music"
         - "r/opensource YouTube Music client"
         - "r/software YouTube Music Mac"
      
      7. **Problems/Issues (3-5 queries):**
         - "YouTube Music desktop app issues"
         - "YTM Mac app crashes"
         - "YouTube Music desktop performance problems"
      
      8. **Year-Specific for Recency (2-3 queries):**
         - "best YouTube Music Mac app 2024"
         - "YouTube Music desktop 2025"
         - "YTM client 2024 recommendations"
      
      9. **Features (3-5 queries):**
         - "YouTube Music offline Mac"
         - "YTM lyrics desktop app"
         - "YouTube Music desktop features"
      
      10. **Developer/GitHub (3-5 queries):**
          - "youtube-music electron app GitHub"
          - "YTM desktop open source"
          - "YouTube Music API desktop client"

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âŒ **BAD EXAMPLE** (DON'T DO THIS - Wastes the tool!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "queries": ["best YouTube Music app"]
      }
      ```
      
      **Why this is BAD:**
      - Only 1 query (minimum is 10!)
      - No diversity (missing 9 other categories)
      - No subreddit targeting
      - No year-specific queries
      - No comparison queries
      - Misses 90% of available consensus data

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âœ… **GOOD EXAMPLE** (DO THIS - Uses tool properly!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "queries": [
          "YouTube Music Mac app",
          "YTM desktop application",
          "best YouTube Music client Mac",
          "recommended YTM desktop",
          "YTMDesktop Mac",
          "th-ch youtube-music",
          "YouTube Music vs Spotify Mac desktop",
          "YTM vs Apple Music desktop",
          "YouTube Music Mac alternative",
          "r/YoutubeMusic desktop app",
          "r/macapps YouTube Music",
          "r/opensource YouTube Music",
          "YouTube Music desktop issues",
          "YTM Mac crashes",
          "best YouTube Music Mac 2024",
          "YouTube Music desktop 2025",
          "YouTube Music offline Mac",
          "YTM lyrics desktop",
          "youtube-music electron GitHub",
          "YTM desktop open source"
        ]
      }
      ```
      
      **Why this is GOOD:**
      - 20 queries (optimal range)
      - Covers ALL 10 categories
      - Includes subreddit targeting (r/YoutubeMusic, r/macapps, r/opensource)
      - Includes year-specific (2024, 2025)
      - Includes comparisons (vs Spotify, vs Apple Music)
      - Includes problems (issues, crashes)
      - Includes features (offline, lyrics)
      - Includes developer angle (GitHub, open source)
      - Each query targets DIFFERENT angle
      - Will find high-consensus posts across multiple perspectives

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ’¡ **PRO TIPS FOR MAXIMUM EFFECTIVENESS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      1. **Use ALL 10 categories** - Each reveals different community perspectives
      2. **Target specific subreddits** - r/YoutubeMusic, r/macapps, r/opensource, r/software
      3. **Include year numbers** - "2024", "2025" for recent discussions
      4. **Add comparison keywords** - "vs", "versus", "compared to", "better than"
      5. **Include problem keywords** - "issue", "bug", "crash", "slow", "problem"
      6. **Vary your phrasing** - "best", "top", "recommended", "popular"
      7. **Use technical terms** - "electron", "GitHub", "API", "open source"
      8. **NO DUPLICATES** - Each query must be unique
      
      **REMEMBER:** More queries = better consensus detection = higher quality results!
      
      **OPERATORS SUPPORTED:**
      - `intitle:` - Search in post titles only
      - `"exact phrase"` - Match exact phrase
      - `OR` - Match either term
      - `-exclude` - Exclude term
      - Auto-adds `site:reddit.com` to all queries

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ§  **ITERATIVE WORKFLOW - THINK â†’ SEARCH â†’ THINK â†’ REFINE â†’ SEARCH AGAIN**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **CRITICAL:** Use sequential thinking BETWEEN tool calls to refine your approach!
      
      **WORKFLOW PATTERN:**
      ```
      1. THINK FIRST (1-2 thoughts via sequentialthinking)
         â†’ Analyze what you need to research
         â†’ Plan your initial query strategy
         â†’ Identify which categories to cover
      
      2. SEARCH (call search_reddit with 10-20 queries)
         â†’ Execute your planned queries
         â†’ Get initial results
      
      3. THINK AFTER RESULTS (2-3 thoughts via sequentialthinking)
         â†’ Evaluate what you found
         â†’ Identify gaps in coverage
         â†’ Notice new angles from results
         â†’ Decide: get_reddit_post OR search_reddit again with refined queries
      
      4. REFINE & ITERATE
         Option A: Get full content
         â†’ Call get_reddit_post with promising URLs
         â†’ Think about insights
         â†’ Search again if gaps remain
         
         Option B: Search again with new angles
         â†’ Call search_reddit with refined queries based on learnings
         â†’ Cover gaps you discovered
         â†’ Think and decide next step
      ```
      
      **WHY THIS WORKS:**
      - Search results = feedback that reveals new angles
      - Thinking between calls = space to evaluate and refine
      - Humans don't search once and stop - neither should you!
      - Initial queries might miss important perspectives
      - Results often reveal better search terms
      
      **EXAMPLE ITERATIVE FLOW:**
      ```
      Step 1: Think
      "I need to research YouTube Music Mac apps. Let me start with direct, 
      recommendation, and comparison queries across 5 subreddits."
      
      Step 2: search_reddit (10 queries)
      [Direct: "YouTube Music Mac", Recommendations: "best YTM Mac", 
      Comparisons: "YTM vs Spotify Mac", etc.]
      
      Step 3: Think (evaluate results)
      "Results show people discussing 'th-ch/youtube-music' and 'YTMDesktop' 
      projects heavily. Also seeing complaints about performance. I should:
      - Search specifically for these project names
      - Add performance-focused queries
      - Target r/opensource and r/electronjs subreddits"
      
      Step 4: search_reddit AGAIN (10 refined queries)
      [Specific: "th-ch youtube-music Mac", "YTMDesktop performance", 
      "r/opensource YouTube Music", "r/electronjs YTM", etc.]
      
      Step 5: Think (evaluate combined results)
      "Now I have comprehensive coverage. Top consensus posts are X, Y, Z.
      Let me fetch full content from these 15 high-consensus posts."
      
      Step 6: get_reddit_post (15 URLs from both searches)
      [Fetch full content + comments from top posts]
      
      Step 7: Think (final synthesis)
      "Based on all results, the community consensus is..."
      ```
      
      **KEY INSIGHT:** Each search reveals new information that should inform your next search!
      
      **MANDATORY WORKFLOW:**
      ```
      search_reddit â†’ sequentialthinking (2-3 thoughts) â†’ 
      EITHER get_reddit_post OR search_reddit again â†’ 
      sequentialthinking â†’ final decision
      ```

    parameters:
      queries:
        type: array
        required: true
        items:
          type: string
        validation:
          minItems: 10  # HARD MINIMUM - enforced
          maxItems: 50  # HARD MAXIMUM - enforced
        description: |
          **PROVIDE 10-50 DIVERSE QUERIES** (Minimum 10 required, 20-30 recommended)
          
          Each query MUST target a different angle. Use the 10-category formula above.
          
          **VALIDATION RULES:**
          - Minimum 10 queries (you'll get an error with less)
          - Maximum 50 queries (you'll get an error with more)
          - Each query should be unique (avoid duplicates)
          - Cover multiple categories from the 10-category formula
          
          **QUICK CHECKLIST:**
          âœ“ At least 10 queries total
          âœ“ Includes direct topic queries
          âœ“ Includes recommendation queries
          âœ“ Includes specific tool names
          âœ“ Includes comparisons (vs, versus)
          âœ“ Includes subreddit targeting (r/...)
          âœ“ Includes year-specific (2024, 2025)
          âœ“ Includes problem keywords (issue, bug, crash)
          âœ“ Includes feature keywords (offline, lyrics, etc.)
          âœ“ Includes developer angle (GitHub, open source)

      date_after:
        type: string
        required: false
        description: "Filter results after date (YYYY-MM-DD format). Optional. Example: '2024-01-01'"

  - name: get_reddit_post
    category: reddit
    capability: reddit
    
    # Configurable limits
    limits:
      min_urls: 2
      max_urls: 50
      recommended_urls: 20
      default_max_comments: 100
    
    description: |
      **ğŸ”¥ FETCH REDDIT POSTS - MAXIMIZE POST COUNT FOR BROAD PERSPECTIVE ğŸ”¥**

      **CRITICAL:** This tool fetches 2-50 Reddit posts with smart comment allocation.
      Using 2-5 posts = MISSING community consensus. Use 10-20+ posts for comprehensive perspective!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“Š **SMART COMMENT BUDGET** (1,000 comments distributed automatically)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - 2 posts: ~500 comments/post (deep dive on specific posts)
      - 10 posts: ~100 comments/post (balanced - GOOD)
      - 20 posts: ~50 comments/post (broad perspective - RECOMMENDED)
      - 50 posts: ~20 comments/post (maximum coverage)
      
      **Comment allocation is AUTOMATIC** - you don't need to calculate!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ¯ **WHEN TO USE DIFFERENT POST COUNTS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **2-5 posts:** Deep dive on specific discussions
      - Use when: You found THE perfect thread and want all comments
      - Trade-off: Deep but narrow perspective
      
      **10-15 posts:** Balanced depth + breadth (GOOD)
      - Use when: Want good comment depth across multiple discussions
      - Trade-off: Good balance
      
      **20-30 posts:** Broad community perspective (RECOMMENDED)
      - Use when: Want to see consensus across many discussions
      - Trade-off: Less comments per post but more diverse opinions
      
      **40-50 posts:** Maximum coverage
      - Use when: Researching controversial topic, need all perspectives
      - Trade-off: Fewer comments per post but comprehensive coverage

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âŒ **BAD EXAMPLE** (DON'T DO THIS - Misses community consensus!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "urls": [
          "https://reddit.com/r/programming/comments/abc123/best_database/"
        ],
        "fetch_comments": true
      }
      ```
      
      **Why this is BAD:**
      - Only 1 URL (minimum is 2, should use 10-20+)
      - Misses other community discussions
      - Single perspective (could be biased/outdated)
      - Not using the tool's multi-post aggregation power

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âœ… **GOOD EXAMPLE** (DO THIS - Gets broad community perspective!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "urls": [
          "https://reddit.com/r/programming/comments/abc123/best_database/",
          "https://reddit.com/r/webdev/comments/def456/database_recommendations/",
          "https://reddit.com/r/node/comments/ghi789/postgresql_vs_mysql/",
          "https://reddit.com/r/golang/comments/jkl012/database_choice/",
          "https://reddit.com/r/rails/comments/mno345/production_database/",
          "https://reddit.com/r/django/comments/pqr678/database_setup/",
          "https://reddit.com/r/dotnet/comments/stu901/database_performance/",
          "https://reddit.com/r/java/comments/vwx234/database_scaling/",
          "https://reddit.com/r/devops/comments/yza567/database_reliability/",
          "https://reddit.com/r/aws/comments/bcd890/rds_vs_aurora/",
          "https://reddit.com/r/selfhosted/comments/efg123/database_hosting/",
          "https://reddit.com/r/sysadmin/comments/hij456/database_backup/",
          "https://reddit.com/r/docker/comments/klm789/database_containers/",
          "https://reddit.com/r/kubernetes/comments/nop012/database_k8s/",
          "https://reddit.com/r/database/comments/qrs345/postgresql_tips/",
          "https://reddit.com/r/sql/comments/tuv678/query_optimization/",
          "https://reddit.com/r/PostgreSQL/comments/wxy901/production_setup/",
          "https://reddit.com/r/mysql/comments/zab234/performance_tuning/",
          "https://reddit.com/r/mongodb/comments/cde567/use_cases/",
          "https://reddit.com/r/redis/comments/fgh890/caching_strategies/"
        ],
        "fetch_comments": true,
        "max_comments": 100
      }
      ```
      
      **Why this is GOOD:**
      - 20 posts (optimal for broad perspective)
      - Covers multiple subreddits (programming, webdev, node, golang, etc.)
      - Different discussion angles (best practices, vs comparisons, production, performance)
      - Will reveal community consensus across diverse communities
      - Automatic comment allocation (~50 comments per post)
      - Comprehensive coverage of the topic

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸš€ **PRO TIPS FOR MAXIMUM EFFECTIVENESS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      1. **Use 10-20+ posts** - More posts = broader community perspective
      2. **Mix subreddits** - Different communities have different perspectives
      3. **Include various discussion types** - Best practices, comparisons, problems, solutions
      4. **Let comment allocation auto-adjust** - Don't override max_comments unless needed
      5. **Use after search_reddit** - Get URLs from search, then fetch full content
      6. **fetch_comments=true** - Comments often have the best insights
      
      **REMEMBER:** More posts = better consensus detection = higher confidence in findings!
      
      **WORKFLOW:** search_reddit (find posts) â†’ get_reddit_post (fetch full content + comments)

    parameters:
      urls:
        type: array
        required: true
        items:
          type: string
        validation:
          minItems: 2
          maxItems: 50
        description: |
          **Reddit post URLs (MINIMUM 2, RECOMMENDED 10-20, MAX 50)**
          
          More posts = broader community perspective and better consensus detection.
          
          **VALIDATION:**
          - Minimum 2 URLs required
          - Maximum 50 URLs allowed
          - Each URL must be a valid Reddit post URL
          
          **RECOMMENDED USAGE:**
          - 2-5 posts: Deep dive on specific discussions
          - 10-15 posts: Balanced depth + breadth
          - 20-30 posts: Broad community perspective (RECOMMENDED)
          - 40-50 posts: Maximum coverage
          
          **PRO TIP:** Get URLs from search_reddit results, then fetch full content here!

      fetch_comments:
        type: boolean
        required: false
        default: true
        description: |
          **Fetch comments from posts (RECOMMENDED: true)**
          
          Comments often contain the BEST insights, solutions, and real-world experiences.
          
          **Set to true (default):** Get post + comments
          **Set to false:** Get post content only (faster but misses insights)
          
          **PRO TIP:** Always keep this true unless you only need post titles/content!

      max_comments:
        type: number
        required: false
        default: 100
        description: |
          **Override automatic comment allocation (DEFAULT: 100, auto-adjusts based on post count)**
          
          Leave empty for smart allocation:
          - 2 posts: ~500 comments/post
          - 10 posts: ~100 comments/post
          - 20 posts: ~50 comments/post
          - 50 posts: ~20 comments/post
          
          **Only override if:** You need specific comment depth
          
          **PRO TIP:** Let the tool auto-allocate for optimal results!

  # ============================================================================
  # DEEP RESEARCH TOOL
  # ============================================================================

  - name: deep_research
    category: research
    capability: deepResearch
    # Complex schema - use existing Zod schema, descriptions injected from YAML
    useZodSchema: true
    zodSchemaRef: "deepResearchParamsSchema"
    
    # Configurable limits
    limits:
      min_questions: 1
      max_questions: 10
      recommended_questions: 5
      min_question_length: 200
      min_specific_questions: 2
    
    description: |
      **ğŸ”¥ BATCH DEEP RESEARCH - USE ALL 10 QUESTION SLOTS FOR COMPREHENSIVE COVERAGE ğŸ”¥**

      **CRITICAL:** This tool runs 2-10 questions IN PARALLEL with AI-powered research.
      Using 1-2 questions = WASTING the parallel research capability!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“Š **TOKEN BUDGET ALLOCATION** (32,000 tokens distributed across questions)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - 2 questions: 16,000 tokens/question (deep dive)
      - 5 questions: 6,400 tokens/question (balanced - RECOMMENDED)
      - 10 questions: 3,200 tokens/question (comprehensive multi-topic)
      
      **All questions research in PARALLEL** - no time penalty for more questions!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ¯ **WHEN TO USE THIS TOOL**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      âœ… **USE for:**
      - Multi-perspective analysis on related topics
      - Researching a domain from multiple angles
      - Validating understanding across different aspects
      - Comparing approaches/technologies side-by-side
      - Deep technical questions requiring comprehensive research
      
      âš ï¸ **ALWAYS ATTACH FILES when asking about:**
      - ğŸ› Bugs/errors â†’ Attach the failing code
      - âš¡ Performance issues â†’ Attach the slow code paths
      - â™»ï¸ Refactoring â†’ Attach current implementation
      - ğŸ” Code review â†’ Attach code to review
      - ğŸ—ï¸ Architecture questions about YOUR code â†’ Attach relevant modules

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“‹ **REQUIRED QUESTION TEMPLATE** (Follow this structure!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **Each question MUST include these sections:**
      
      **1. ğŸ¯ WHAT I NEED:**
      [Clearly state what you're trying to achieve, solve, or understand]
      
      **2. ğŸ¤” WHY I'M RESEARCHING THIS:**
      [Explain the context - what decision does this inform? What problem are you solving?]
      
      **3. ğŸ“š WHAT I ALREADY KNOW:**
      [Share your current understanding so research fills gaps, not repeats basics]
      
      **4. ğŸ”§ HOW I PLAN TO USE THIS:**
      [Describe the practical application - implementation, debugging, architecture, etc.]
      
      **5. â“ SPECIFIC QUESTIONS (2-5):**
      - Question 1: [Specific, pointed question]
      - Question 2: [Another specific question]
      - Question 3: [etc.]
      
      **6. ğŸŒ PRIORITY SOURCES (optional):**
      [Sites/docs to prioritize: "Prefer official React docs, GitHub issues, Stack Overflow"]
      
      **7. âš¡ PRIORITY INFO (optional):**
      [What matters most: "Focus on performance implications" or "Prioritize security best practices"]

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âŒ **BAD EXAMPLE** (DON'T DO THIS - Wastes the tool!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "questions": [
          {
            "question": "Research React hooks"
          }
        ]
      }
      ```
      
      **Why this is BAD:**
      - Only 1 question (should use 5-10 for comprehensive coverage)
      - Too vague ("Research React hooks")
      - No template sections (missing WHY, WHAT I KNOW, etc.)
      - No specific sub-questions
      - No file attachments (if this is about YOUR code)
      - Wastes 90% of available research capacity

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âœ… **GOOD EXAMPLE** (DO THIS - Uses tool properly!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "questions": [
          {
            "question": "ğŸ¯ WHAT I NEED: Understand when to use useCallback vs useMemo in React 18\n\nğŸ¤” WHY: Optimizing a data-heavy dashboard with 50+ components, seeing performance issues\n\nğŸ“š WHAT I KNOW: Both memoize values, useCallback for functions, useMemo for computed values. Unclear when each prevents re-renders.\n\nğŸ”§ HOW I'LL USE THIS: Refactor Dashboard.tsx to eliminate unnecessary re-renders\n\nâ“ SPECIFIC QUESTIONS:\n1. When does useCallback actually prevent re-renders vs when it doesn't?\n2. Performance benchmarks: useCallback vs useMemo vs neither in React 18?\n3. Common anti-patterns that negate their benefits?\n4. How to measure if they're actually helping?\n5. Best practices for large component trees?\n\nğŸŒ PRIORITY: Official React docs, React team blog posts, performance case studies\nâš¡ FOCUS: Patterns for frequently updating state"
          },
          {
            "question": "ğŸ¯ WHAT I NEED: Best practices for React Context API with frequent updates\n\nğŸ¤” WHY: Dashboard uses Context for filter state, causing full re-renders\n\nğŸ“š WHAT I KNOW: Context triggers re-render of all consumers. Can split contexts or use useMemo.\n\nğŸ”§ HOW I'LL USE THIS: Redesign FilterContext to minimize re-renders\n\nâ“ SPECIFIC QUESTIONS:\n1. How to structure Context to avoid unnecessary re-renders?\n2. When to split one Context into multiple?\n3. Context + useReducer vs external state library?\n4. Performance comparison: Context vs Zustand vs Redux?\n\nğŸŒ PRIORITY: React docs, Kent C. Dodds articles, real-world examples\nâš¡ FOCUS: Patterns for frequently updating state"
          },
          {
            "question": "ğŸ¯ WHAT I NEED: Virtualization strategy for rendering 10,000+ rows\n\nğŸ¤” WHY: DataGrid component freezes when displaying large datasets\n\nğŸ“š WHAT I KNOW: react-window and react-virtualized exist. Need to understand tradeoffs.\n\nğŸ”§ HOW I'LL USE THIS: Implement virtualization in DataGrid.tsx\n\nâ“ SPECIFIC QUESTIONS:\n1. react-window vs react-virtualized in 2024?\n2. How to handle dynamic row heights?\n3. Integration with React 18 concurrent features?\n4. Performance impact of virtualization overhead?\n\nğŸŒ PRIORITY: Library docs, performance benchmarks, production examples"
          }
        ]
      }
      ```
      
      **Why this is GOOD:**
      - 3 questions (good coverage, could use up to 10)
      - Each follows the template structure
      - Specific context and WHY for each
      - 2-5 specific sub-questions per topic
      - File attachments for code-related question
      - Detailed file descriptions
      - Focus areas specified
      - Will get comprehensive, actionable research

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸš€ **PRO TIPS FOR MAXIMUM EFFECTIVENESS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      1. **Use 5-10 questions** - Maximize parallel research capacity
      2. **Follow the template** - Include all 7 sections for each question
      3. **Be specific** - Include version numbers, error codes, library names
      4. **Add 2-5 sub-questions** - Break down what you need to know
      5. **Attach files for code questions** - MANDATORY for bugs/performance/refactoring
      6. **Describe files thoroughly** - Use numbered sections [1] [2] [3] [4] [5]
      7. **Specify focus areas** - "Focus on X, Y, Z" for prioritization
      8. **Group related questions** - Research a domain from multiple angles
      
      **REMEMBER:** More questions = more comprehensive research = better decisions!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ§  **ITERATIVE WORKFLOW - THINK â†’ RESEARCH â†’ THINK â†’ EXPAND â†’ RESEARCH AGAIN**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **CRITICAL:** Use sequential thinking BETWEEN research calls to expand scope based on findings!
      
      **WORKFLOW PATTERN:**
      ```
      1. THINK FIRST (1-2 thoughts via sequentialthinking)
         â†’ Analyze what you need to research
         â†’ Plan initial 3-5 questions
         â†’ Identify knowledge gaps
      
      2. RESEARCH (call deep_research with 3-5 questions)
         â†’ Execute your planned questions
         â†’ Get comprehensive research results
      
      3. THINK AFTER RESULTS (2-3 thoughts via sequentialthinking)
         â†’ Evaluate what you learned
         â†’ Identify NEW questions that emerged
         â†’ Notice gaps or deeper areas to explore
         â†’ Decide: sufficient OR need more research
      
      4. EXPAND SCOPE & ITERATE (if gaps found)
         â†’ Call deep_research AGAIN with 3-5 NEW questions
         â†’ Based on learnings from first research
         â†’ Explore deeper or adjacent topics
         â†’ Think and synthesize
      ```
      
      **WHY THIS WORKS:**
      - Research results reveal questions you didn't know to ask
      - Thinking between calls = space to digest and identify gaps
      - Initial questions might be too narrow or miss key aspects
      - Results often reveal more important questions
      - Iterative deepening = comprehensive understanding
      
      **EXAMPLE ITERATIVE FLOW:**
      ```
      Step 1: Think
      "Need to understand React performance optimization. Start with 
      memoization, Context API, and virtualization questions."
      
      Step 2: deep_research (3 questions)
      [Q1: useCallback vs useMemo, Q2: Context API patterns, 
       Q3: Virtualization libraries]
      
      Step 3: Think (evaluate results)
      "Research revealed:
      - React 18 concurrent features are important for performance
      - Suspense + transitions affect optimization strategy
      - Server Components change the game
      
      NEW questions emerged:
      - How do concurrent features interact with memoization?
      - When to use Server Components vs Client Components?
      - Suspense boundaries impact on performance?
      
      Should research these deeper aspects now."
      
      Step 4: deep_research AGAIN (3 NEW questions)
      [Q1: React 18 concurrent features + memoization,
       Q2: Server vs Client Components performance,
       Q3: Suspense boundaries optimization]
      
      Step 5: Think (evaluate combined results)
      "Now have complete picture. Initial research gave fundamentals,
      second research covered advanced patterns. Ready to implement."
      ```
      
      **KEY INSIGHT:** First research reveals what you SHOULD have asked!
      
      **SCOPE EXPANSION TRIGGERS:**
      - Results mention concepts you didn't research
      - Answers raise new questions
      - Realize initial scope was too narrow
      - Discover related topics that matter
      - Need deeper understanding of specific aspect
      
      **MANDATORY WORKFLOW:**
      ```
      deep_research (3-5 questions) â†’ 
      sequentialthinking (2-3 thoughts, evaluate results) â†’
      OPTIONAL: deep_research again (3-5 NEW questions based on learnings) â†’
      sequentialthinking (synthesize) â†’
      final decision
      ```
      
      **REMEMBER:**
      - ALWAYS think after getting results (digest and identify gaps!)
      - DON'T assume first research is complete (iterate based on findings!)
      - USE learnings to ask better questions (results = feedback!)
      - EXPAND scope when results reveal new important areas!

    # Schema descriptions to inject into existing Zod schema
    schemaDescriptions:
      urls: |
        **URLs to scrape (1-50 URLs recommended: 3-5 for balanced depth/breadth)**
        
        More URLs = broader coverage but fewer tokens per URL
        - 3 URLs: ~10K tokens each (deep extraction)
        - 5 URLs: ~6K tokens each (balanced - RECOMMENDED)
        - 10 URLs: ~3K tokens each (detailed)
        - 50 URLs: ~640 tokens each (quick scan)
        
        **VALIDATION:**
        - Minimum 1 URL required
        - Maximum 50 URLs allowed
        - Each URL must be valid HTTP/HTTPS

      timeout: "Timeout in seconds for each URL (5-120 seconds, default: 30)"

      use_llm: |
        **Enable AI-powered content extraction (HIGHLY RECOMMENDED - set to true)**
        
        âš¡ **ALWAYS SET THIS TO true FOR INTELLIGENT EXTRACTION** âš¡
        
        **Benefits of use_llm=true:**
        - Filters out navigation, ads, footers automatically
        - Extracts ONLY what you specify in what_to_extract
        - Handles complex page structures intelligently
        - Returns clean, structured content
        - Saves manual HTML parsing
        
        **Cost:** ~$0.001 per page (pennies for quality extraction)
        **When to use false:** Only for debugging or raw HTML needs
        
        **Default:** false (but you should set it to true!)

      what_to_extract: |
        **Extraction prompt for AI (REQUIRED when use_llm=true)**
        
        **FORMULA:** Extract [target1] | [target2] | [target3] with focus on [aspect1], [aspect2]
        
        **REQUIREMENTS:**
        - Minimum 50 characters (be detailed!)
        - Minimum 3 extraction targets separated by `|`
        - Include "with focus on" for prioritization
        - Be SPECIFIC about what you want
        
        **EXAMPLES:**
        
        âœ… GOOD: "Extract pricing tiers | plan features | API rate limits | enterprise options | 
        integration capabilities | user testimonials | technical requirements | performance benchmarks 
        with focus on enterprise features, API limitations, and real-world performance data"
        
        âŒ BAD: "get main content" (too vague, no targets, no focus)
        
        **PRO TIP:** More specific targets = better extraction quality!

  # ============================================================================
  # SCRAPE LINKS TOOL
  # ============================================================================

  - name: scrape_links
    category: scrape
    capability: scraping
    useZodSchema: true
    zodSchemaRef: "scrapeLinksParamsSchema"
    
    # Configurable limits
    limits:
      min_urls: 1
      max_urls: 50
      recommended_urls: 5
      min_extraction_prompt_length: 50
      min_extraction_targets: 3
    
    description: |
      **ğŸ”¥ INTELLIGENT WEB SCRAPING - ALWAYS USE AI EXTRACTION ğŸ”¥**

      **CRITICAL:** This tool has TWO modes:
      1. **Basic scraping** (use_llm=false) - Gets raw HTML/text
      2. **AI-powered extraction** (use_llm=true) - Intelligently extracts what you need â­ **USE THIS!**

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âš¡ **ALWAYS SET use_llm=true FOR INTELLIGENT EXTRACTION** âš¡
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **Why use AI extraction:**
      - Filters out navigation, ads, footers automatically
      - Extracts ONLY what you specify
      - Handles complex page structures
      - Returns clean, structured content
      - Saves you from parsing HTML manually
      
      **Cost:** Minimal (~$0.01 per 10 pages with quality extraction)
      **Benefit:** 10x better results than raw scraping

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“Š **TOKEN ALLOCATION** (32,000 tokens distributed across URLs)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - 3 URLs: ~10,666 tokens/URL (deep extraction)
      - 5 URLs: ~6,400 tokens/URL (balanced - RECOMMENDED)
      - 10 URLs: ~3,200 tokens/URL (detailed)
      - 50 URLs: ~640 tokens/URL (high-level scan)
      
      **AUTOMATIC FALLBACK:** Basic â†’ JavaScript rendering â†’ JavaScript + US geo-targeting
      **BATCHING:** Max 30 concurrent requests (50 URLs = [30] then [20] batches)

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ¯ **EXTRACTION PROMPT FORMULA** (Use OR statements for multiple targets!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **Template:**
      ```
      Extract [target1] | [target2] | [target3] | [target4] | [target5]
      with focus on [aspect1], [aspect2], [aspect3]
      ```
      
      **Rules:**
      - Use pipe `|` to separate extraction targets (minimum 3 targets)
      - Be SPECIFIC about what you want
      - Include "with focus on" for prioritization
      - More targets = more comprehensive extraction
      - Aim for 5-10 extraction targets

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âŒ **BAD EXAMPLE** (DON'T DO THIS - Wastes AI extraction!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "urls": ["https://example.com/pricing"],
        "use_llm": false,
        "what_to_extract": "get pricing"
      }
      ```
      
      **Why this is BAD:**
      - use_llm=false (missing AI extraction!)
      - Vague extraction prompt ("get pricing")
      - Only 1 extraction target
      - No focus areas specified
      - Will return messy HTML instead of clean data

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âœ… **GOOD EXAMPLE** (DO THIS - Uses AI extraction properly!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "urls": [
          "https://example.com/pricing",
          "https://example.com/features",
          "https://example.com/docs/api",
          "https://example.com/enterprise",
          "https://example.com/testimonials"
        ],
        "use_llm": true,
        "what_to_extract": "Extract pricing tiers | plan features | API rate limits | enterprise options | 
        integration capabilities | user testimonials | technical requirements | performance benchmarks with focus on enterprise features, API limitations, and real-world performance data"
      }
      ```
      
      **Why this is GOOD:**
      - use_llm=true âœ“ (AI extraction enabled!)
      - 5 URLs (balanced depth/breadth)
      - 8 extraction targets separated by `|`
      - Specific targets (pricing tiers, API rate limits, etc.)
      - Focus areas specified (enterprise, API, performance)
      - Will return clean, structured extraction
      - Covers multiple aspects of the product

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ’¡ **EXTRACTION PROMPT EXAMPLES BY USE CASE**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **Product Research:**
      ```
      Extract pricing details | feature comparisons | user reviews | technical specifications | 
      integration options | support channels | deployment models | security features 
      with focus on enterprise capabilities, pricing transparency, and integration complexity
      ```
      
      **Technical Documentation:**
      ```
      Extract API endpoints | authentication methods | rate limits | error codes | 
      request examples | response schemas | SDK availability | webhook support 
      with focus on authentication flow, rate limiting policies, and error handling patterns
      ```
      
      **Competitive Analysis:**
      ```
      Extract product features | pricing models | target customers | unique selling points | 
      technology stack | customer testimonials | case studies | market positioning 
      with focus on differentiators, pricing strategy, and customer satisfaction
      ```
      
      **Content Research:**
      ```
      Extract main arguments | supporting evidence | data points | expert quotes | 
      methodology | conclusions | references | publication date 
      with focus on credibility, data quality, and actionable insights
      ```

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸš€ **PRO TIPS FOR MAXIMUM EFFECTIVENESS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      1. **ALWAYS use use_llm=true** - The AI extraction is the tool's superpower
      2. **Use 3-10 URLs** - Balance between depth and breadth
      3. **Specify 5-10 extraction targets** - More targets = more comprehensive
      4. **Use pipe `|` separators** - Clearly separate each target
      5. **Add focus areas** - "with focus on X, Y, Z" for prioritization
      6. **Be specific** - "pricing tiers" not "pricing", "API rate limits" not "API info"
      7. **Think parallel extraction** - Each target extracts independently
      8. **Cover multiple aspects** - Features, pricing, technical, social proof
      
      **REMEMBER:** AI extraction costs pennies but saves hours of manual parsing!

    schemaDescriptions:
      urls: |
        **URLs to scrape (1-50 URLs recommended: 3-5 for balanced depth/breadth)**
        
        More URLs = broader coverage but fewer tokens per URL
        - 3 URLs: ~10K tokens each (deep extraction)
        - 5 URLs: ~6K tokens each (balanced - RECOMMENDED)
        - 10 URLs: ~3K tokens each (detailed)
        - 50 URLs: ~640 tokens each (quick scan)
        
        **VALIDATION:**
        - Minimum 1 URL required
        - Maximum 50 URLs allowed
        - Each URL must be valid HTTP/HTTPS

      timeout: "Timeout in seconds for each URL (5-120 seconds, default: 30)"

      use_llm: |
        **Enable AI-powered content extraction (HIGHLY RECOMMENDED - set to true)**
        
        âš¡ **ALWAYS SET THIS TO true FOR INTELLIGENT EXTRACTION** âš¡
        
        **Benefits of use_llm=true:**
        - Filters out navigation, ads, footers automatically
        - Extracts ONLY what you specify in what_to_extract
        - Handles complex page structures intelligently
        - Returns clean, structured content
        - Saves manual HTML parsing
        
        **Cost:** ~$0.001 per page (pennies for quality extraction)
        **When to use false:** Only for debugging or raw HTML needs
        
        **Default:** false (but you should set it to true!)

      what_to_extract: |
        **Extraction prompt for AI (REQUIRED when use_llm=true)**
        
        **FORMULA:** Extract [target1] | [target2] | [target3] with focus on [aspect1], [aspect2]
        
        **REQUIREMENTS:**
        - Minimum 50 characters (be detailed!)
        - Minimum 3 extraction targets separated by `|`
        - Include "with focus on" for prioritization
        - Be SPECIFIC about what you want
        
        **EXAMPLES:**
        
        âœ… GOOD: "Extract pricing tiers | plan features | API rate limits | enterprise options | 
        integration capabilities | user testimonials | technical requirements | performance benchmarks 
        with focus on enterprise features, API limitations, and real-world performance data"
        
        âŒ BAD: "get main content" (too vague, no targets, no focus)
        
        **PRO TIP:** More specific targets = better extraction quality!

  # ============================================================================
  # WEB SEARCH TOOL
  # ============================================================================

  - name: web_search
    category: search
    capability: search
    useZodSchema: true
    zodSchemaRef: "webSearchParamsSchema"
    
    # Configurable limits
    limits:
      min_keywords: 3
      max_keywords: 100
      recommended_keywords: 7
    
    description: |
      **ğŸ”¥ BATCH WEB SEARCH - MINIMUM 3 KEYWORDS, RECOMMENDED 5-7 ğŸ”¥**

      **CRITICAL:** This tool searches up to 100 keywords IN PARALLEL via Google.
      Using 1-2 keywords = WASTING the tool's parallel search power!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ“Š **KEYWORD BUDGET** (Use multiple perspectives!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - **MINIMUM:** 3 keywords (hard requirement)
      - **RECOMMENDED:** 5-7 keywords (optimal diversity)
      - **MAXIMUM:** 100 keywords (comprehensive research)
      
      **RESULTS:** 10 results per keyword, all searches run in parallel
      - 3 keywords = 30 total results
      - 7 keywords = 70 total results  
      - 100 keywords = 1000 total results

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ¯ **KEYWORD DIVERSITY FORMULA** (Cover different angles!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      Each keyword should target a DIFFERENT perspective:
      
      1. **Direct/Broad:** "React state management"
      2. **Specific/Technical:** "React useReducer vs Redux"
      3. **Problem-Focused:** "React state management performance issues"
      4. **Best Practices:** "React state management best practices 2024"
      5. **Comparison:** "React state management libraries comparison"
      6. **Tutorial/Guide:** "React state management tutorial"
      7. **Advanced:** "React state management patterns large applications"

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ”§ **SEARCH OPERATORS** (Use these for precision!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      - `site:domain.com` - Search within specific site
      - `"exact phrase"` - Match exact phrase
      - `-exclude` - Exclude term
      - `filetype:pdf` - Find specific file types
      - `OR` - Match either term
      
      **Examples:**
      - `"React hooks" site:github.com` - React hooks repos on GitHub
      - `React state management -Redux` - Exclude Redux results
      - `React tutorial filetype:pdf` - Find React PDF tutorials
      - `React OR Vue state management` - Compare frameworks

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âŒ **BAD EXAMPLE** (DON'T DO THIS - Wastes parallel search!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "keywords": ["React"]
      }
      ```
      
      **Why this is BAD:**
      - Only 1 keyword (minimum is 3!)
      - Too broad/vague
      - No search operators
      - No diversity (missing 6 other perspectives)
      - Misses specific, technical, and comparison results

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      âœ… **GOOD EXAMPLE** (DO THIS - Uses parallel search properly!)
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      ```json
      {
        "keywords": [
          "React state management best practices",
          "React useReducer vs Redux 2024",
          "React Context API performance",
          "Zustand React state library",
          "React state management large applications",
          "\"React state\" site:github.com",
          "React global state alternatives -Redux"
        ]
      }
      ```
      
      **Why this is GOOD:**
      - 7 keywords (optimal range)
      - Covers multiple perspectives (best practices, comparison, performance, specific library)
      - Uses search operators (site:, "exact", -exclude, filetype:)
      - Includes year for recency (2024)
      - Targets different aspects (performance, large apps, alternatives)
      - Each keyword reveals different insights
      - Will find diverse, high-quality results

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ’¡ **KEYWORD CRAFTING STRATEGIES BY USE CASE**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **Technology Research:**
      ```json
      [
        "PostgreSQL vs MySQL performance 2024",
        "PostgreSQL best practices production",
        "\"PostgreSQL\" site:github.com stars:>1000",
        "PostgreSQL connection pooling",
        "PostgreSQL performance tuning",
        "PostgreSQL vs MongoDB use cases",
        "PostgreSQL replication setup"
      ]
      ```
      
      **Problem Solving:**
      ```json
      [
        "Docker container memory leak debugging",
        "Docker memory limit not working",
        "\"Docker OOM\" site:stackoverflow.com",
        "Docker container resource monitoring",
        "Docker memory optimization best practices"
      ]
      ```
      
      **Comparison Research:**
      ```json
      [
        "Next.js vs Remix performance",
        "Next.js 14 vs Remix 2024",
        "\"Next.js\" OR \"Remix\" benchmarks",
        "Next.js Remix migration guide",
        "Next.js vs Remix developer experience"
      ]
      ```

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸš€ **PRO TIPS FOR MAXIMUM EFFECTIVENESS**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      1. **Use 5-7 keywords minimum** - Each reveals different perspective
      2. **Add year numbers** - "2024", "2025" for recent content
      3. **Use search operators** - site:, "exact", -exclude, filetype:
      4. **Vary specificity** - Mix broad + specific keywords
      5. **Include comparisons** - "vs", "versus", "compared to", "OR"
      6. **Target sources** - site:github.com, site:stackoverflow.com
      7. **Add context** - "best practices", "tutorial", "production", "performance"
      8. **Think parallel** - Each keyword searches independently
      
      **REMEMBER:** More diverse keywords = better coverage = higher quality results!
      
      **FOLLOW-UP:** Use `scrape_links` to extract full content from promising URLs!

      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      ğŸ§  **ITERATIVE WORKFLOW - THINK â†’ SEARCH â†’ THINK â†’ SCRAPE â†’ THINK â†’ REFINE**
      â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
      
      **CRITICAL:** Use sequential thinking BETWEEN tool calls to refine based on results!
      
      **WORKFLOW PATTERN:**
      ```
      1. THINK FIRST (1-2 thoughts via sequentialthinking)
         â†’ Analyze what you need to find
         â†’ Plan initial keyword strategy
         â†’ Identify which perspectives to cover
      
      2. SEARCH (call web_search with 5-7 keywords)
         â†’ Execute your planned keywords
         â†’ Get search results with URLs
      
      3. THINK AFTER RESULTS (2-3 thoughts via sequentialthinking)
         â†’ Evaluate which URLs look most promising
         â†’ Identify gaps in search coverage
         â†’ Notice new angles from result snippets
         â†’ Decide: scrape_links OR search again OR both
      
      4. SCRAPE PROMISING URLs (call scrape_links)
         â†’ Extract full content from 3-10 best URLs
         â†’ Use AI extraction (use_llm=true)
         â†’ Get detailed information
      
      5. THINK AFTER SCRAPING (2-3 thoughts via sequentialthinking)
         â†’ Evaluate scraped content
         â†’ Identify what's still missing
         â†’ Decide if more searches needed
      
      6. REFINE & ITERATE (if gaps remain)
         â†’ Call web_search AGAIN with refined keywords
         â†’ Cover gaps discovered from initial results
         â†’ Scrape new URLs
         â†’ Think and synthesize
      ```
      
      **WHY THIS WORKS:**
      - Search results reveal new keywords you didn't think of
      - Scraped content shows what's actually useful vs what looked good
      - Thinking between tool calls prevents tunnel vision
      - Iterative refinement = comprehensive coverage
      - Humans don't search once - neither should you!
      
      **EXAMPLE ITERATIVE FLOW:**
      ```
      Step 1: Think
      "Need to research PostgreSQL vs MySQL. Start with performance, 
      scalability, and production use cases."
      
      Step 2: web_search (5 keywords)
      ["PostgreSQL vs MySQL performance 2024", 
       "PostgreSQL scalability production",
       "PostgreSQL vs MySQL benchmarks",
       "PostgreSQL production best practices",
       "MySQL vs PostgreSQL use cases"]
      
      Step 3: Think (evaluate results)
      "Results mention connection pooling, replication, and specific 
      benchmarks. I see URLs from official docs, DigitalOcean, and 
      Percona. Should scrape these for details. Also noticed 'PgBouncer' 
      mentioned - should search for that specifically."
      
      Step 4: scrape_links (5 URLs from results)
      [Scrape PostgreSQL docs, MySQL docs, benchmark articles]
      
      Step 5: Think (evaluate scraped content)
      "Got good info on connection pooling and replication. But missing:
      - Specific PgBouncer vs MySQL connection pooling comparison
      - Real-world migration experiences
      - Cost implications at scale
      Need to search again with these angles."
      
      Step 6: web_search AGAIN (5 refined keywords)
      ["PgBouncer vs MySQL connection pooling",
       "PostgreSQL vs MySQL migration experience",
       "PostgreSQL vs MySQL cost at scale",
       "PostgreSQL vs MySQL production stories",
       "PostgreSQL MySQL real-world comparison"]
      
      Step 7: scrape_links (new URLs)
      [Scrape migration stories, cost analyses]
      
      Step 8: Think (final synthesis)
      "Now have complete picture: performance, scalability, migration 
      experiences, costs. Can make informed recommendation."
      ```
      
      **KEY INSIGHT:** Results are feedback! Use them to discover better searches!
      
      **MANDATORY WORKFLOW:**
      ```
      web_search â†’ sequentialthinking (2-3 thoughts) â†’ 
      scrape_links (MUST scrape promising URLs) â†’
      sequentialthinking (evaluate) â†’
      OPTIONAL: web_search again if gaps found â†’
      sequentialthinking â†’ final synthesis
      ```
      
      **REMEMBER:** 
      - ALWAYS scrape after web_search (that's where the real content is!)
      - ALWAYS think between tool calls (evaluate and refine!)
      - DON'T stop after one search (iterate based on learnings!)

    schemaDescriptions:
      keywords: |
        **Array of search keywords (MINIMUM 3, RECOMMENDED 5-7, MAX 100)**
        
        Each keyword runs as a separate Google search in parallel.
        
        **VALIDATION:**
        - Minimum 3 keywords required
        - Maximum 100 keywords allowed
        - Each keyword should target different angle
        
        **DIVERSITY CHECKLIST:**
        âœ“ Includes broad keyword
        âœ“ Includes specific/technical keyword
        âœ“ Includes comparison keyword (vs, OR)
        âœ“ Includes best practices keyword
        âœ“ Includes year-specific keyword (2024, 2025)
        âœ“ Uses search operators (site:, "exact", -exclude)
        âœ“ Targets specific sources (GitHub, Stack Overflow, docs)
        
        **SEARCH OPERATORS:**
        - `site:domain.com` - Search within site
        - `"exact phrase"` - Match exact phrase
        - `-exclude` - Exclude term
        - `filetype:pdf` - Find file type
        - `OR` - Match either term
        
        **EXAMPLES:**
        
        âœ… GOOD: ["React hooks best practices", "React useEffect vs useLayoutEffect", 
        "\"React hooks\" site:github.com", "React hooks performance 2024", 
        "React custom hooks patterns", "React hooks -class components"]
        
        âŒ BAD: ["React"] (too vague, only 1 keyword, no diversity)
