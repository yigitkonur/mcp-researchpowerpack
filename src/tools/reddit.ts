/**
 * Reddit Tools - Search and Fetch
 * NEVER throws - always returns structured response for graceful degradation
 */

import { SearchClient } from '../clients/search.js';
import { RedditClient, calculateCommentAllocation, type PostResult, type Comment } from '../clients/reddit.js';
import { aggregateAndRankReddit, generateRedditEnhancedOutput } from '../utils/url-aggregator.js';
import { REDDIT } from '../config/index.js';
import { classifyError } from '../utils/errors.js';
import { createLLMProcessor, processContentWithLLM } from '../services/llm-processor.js';
import { getToolConfig } from '../config/loader.js';
import {
  mcpLog,
  formatSuccess,
  formatError,
  formatBatchHeader,
  TOKEN_BUDGETS,
} from './utils.js';

// ============================================================================
// Formatters
// ============================================================================

function formatComments(comments: Comment[]): string {
  let md = '';
  for (const c of comments) {
    const indent = '  '.repeat(c.depth);
    const op = c.isOP ? ' **[OP]**' : '';
    const score = c.score >= 0 ? `+${c.score}` : `${c.score}`;
    md += `${indent}- **u/${c.author}**${op} _(${score})_\n`;
    const bodyLines = c.body.split('\n').map(line => `${indent}  ${line}`).join('\n');
    md += `${bodyLines}\n\n`;
  }
  return md;
}

function formatPost(result: PostResult, fetchComments: boolean): string {
  const { post, comments, allocatedComments } = result;
  let md = `## ${post.title}\n\n`;
  md += `**r/${post.subreddit}** ‚Ä¢ u/${post.author} ‚Ä¢ ‚¨ÜÔ∏è ${post.score} ‚Ä¢ üí¨ ${post.commentCount} comments\n`;
  md += `üîó ${post.url}\n\n`;

  if (post.body) {
    md += `### Post Content\n\n${post.body}\n\n`;
  }

  if (fetchComments && comments.length > 0) {
    md += `### Top Comments (${comments.length}/${post.commentCount} shown, allocated: ${allocatedComments})\n\n`;
    md += formatComments(comments);
  } else if (!fetchComments) {
    md += `_Comments not fetched (fetch_comments=false)_\n\n`;
  }

  return md;
}

// ============================================================================
// Search Reddit Handler
// ============================================================================

export async function handleSearchReddit(
  queries: string[],
  apiKey: string,
  dateAfter?: string
): Promise<string> {
  try {
    const limited = queries.slice(0, 50);
    const client = new SearchClient(apiKey);
    const results = await client.searchRedditMultiple(limited, dateAfter);

    // Check if any results were found
    let totalResults = 0;
    for (const items of results.values()) {
      totalResults += items.length;
    }

    if (totalResults === 0) {
      return formatError({
        code: 'NO_RESULTS',
        message: `No results found for any of the ${limited.length} queries`,
        toolName: 'search_reddit',
        howToFix: [
          'Try broader or simpler search terms',
          'Check spelling of technical terms',
          'Remove date filters if using them',
        ],
        alternatives: [
          'web_search(keywords=["topic best practices", "topic guide", "topic recommendations 2025"]) ‚Äî get results from the broader web instead',
          'scrape_links(urls=[...any URLs you already have...], use_llm=true) ‚Äî if you have URLs from earlier searches, scrape them now',
          'deep_research(questions=[{question: "What are the key findings about [topic]?"}]) ‚Äî synthesize from AI research',
        ],
      });
    }

    // Aggregate and rank results by CTR
    const aggregation = aggregateAndRankReddit(results, 3);

    // Generate enhanced output with consensus highlighting AND per-query raw results
    return generateRedditEnhancedOutput(aggregation, limited, results);
  } catch (error) {
    const structuredError = classifyError(error);
    return formatError({
      code: structuredError.code,
      message: structuredError.message,
      retryable: structuredError.retryable,
      toolName: 'search_reddit',
      howToFix: ['Verify SERPER_API_KEY is set correctly'],
      alternatives: [
        'web_search(keywords=["topic recommendations", "topic best practices", "topic vs alternatives"]) ‚Äî uses the same API key, but try anyway as it may work for general search',
        'deep_research(questions=[{question: "What does the community recommend for [topic]?"}]) ‚Äî uses a different API (OpenRouter), not affected by this error',
        'scrape_links(urls=[...any URLs you already have...], use_llm=true) ‚Äî if you have URLs from prior steps, scrape them now',
      ],
    });
  }
}

// ============================================================================
// Get Reddit Posts Handler
// ============================================================================

interface GetRedditPostsOptions {
  fetchComments?: boolean;
  maxCommentsOverride?: number;
  use_llm?: boolean;
  what_to_extract?: string;
}

// Get extraction suffix from YAML config (fallback to hardcoded if not found)
function getExtractionSuffix(): string {
  const config = getToolConfig('get_reddit_post');
  const suffix = config?.limits?.extraction_suffix;
  if (typeof suffix === 'string') return suffix;
  return `
---

‚ö†Ô∏è IMPORTANT: Extract and synthesize the key insights, opinions, and recommendations from these Reddit discussions. Focus on:
- Common themes and consensus across posts
- Specific recommendations with context
- Contrasting viewpoints and debates
- Real-world experiences and lessons learned
- Technical details and implementation tips

Be comprehensive but concise. Prioritize actionable insights.

---`;
}

function enhanceExtractionInstruction(instruction: string | undefined): string {
  const base = instruction || 'Extract key insights, recommendations, and community consensus from these Reddit discussions.';
  return `${base}\n\n${getExtractionSuffix()}`;
}

export async function handleGetRedditPosts(
  urls: string[],
  clientId: string,
  clientSecret: string,
  maxComments = 100,
  options: GetRedditPostsOptions = {}
): Promise<string> {
  try {
    const { fetchComments = true, maxCommentsOverride, use_llm = false, what_to_extract } = options;

    if (urls.length < REDDIT.MIN_POSTS) {
      return formatError({
        code: 'MIN_POSTS',
        message: `Minimum ${REDDIT.MIN_POSTS} Reddit posts required. Received: ${urls.length}`,
        toolName: 'get_reddit_post',
        howToFix: [`Add at least ${REDDIT.MIN_POSTS - urls.length} more Reddit URL(s)`],
        alternatives: [
          `search_reddit(queries=["topic discussion", "topic recommendations", "topic experiences"]) ‚Äî find more Reddit posts first, then call get_reddit_post with ${REDDIT.MIN_POSTS}+ URLs`,
        ],
      });
    }
    if (urls.length > REDDIT.MAX_POSTS) {
      return formatError({
        code: 'MAX_POSTS',
        message: `Maximum ${REDDIT.MAX_POSTS} Reddit posts allowed. Received: ${urls.length}`,
        toolName: 'get_reddit_post',
        howToFix: [`Remove ${urls.length - REDDIT.MAX_POSTS} URL(s) and retry`],
      });
    }

    const allocation = calculateCommentAllocation(urls.length);
    const commentsPerPost = fetchComments ? (maxCommentsOverride || allocation.perPostCapped) : 0;
    const totalBatches = Math.ceil(urls.length / REDDIT.BATCH_SIZE);

    const client = new RedditClient(clientId, clientSecret);
    const batchResult = await client.batchGetPosts(urls, commentsPerPost, fetchComments);
    const results = batchResult.results;

    // Initialize LLM processor if needed
    const llmProcessor = use_llm ? createLLMProcessor() : null;
    const tokensPerUrl = use_llm ? Math.floor(TOKEN_BUDGETS.RESEARCH / urls.length) : 0;
    const enhancedInstruction = use_llm ? enhanceExtractionInstruction(what_to_extract) : undefined;

    let successful = 0;
    let failed = 0;
    let llmErrors = 0;
    const contents: string[] = [];

    for (const [url, result] of results) {
      if (result instanceof Error) {
        failed++;
        contents.push(`## ‚ùå Failed: ${url}\n\n_${result.message}_`);
      } else {
        successful++;
        let postContent = formatPost(result, fetchComments);

        // Apply LLM extraction per-URL if enabled
        if (use_llm && llmProcessor) {
          mcpLog('info', `[${successful}/${urls.length}] Applying LLM extraction to ${url}`, 'reddit');

          const llmResult = await processContentWithLLM(
            postContent,
            { use_llm: true, what_to_extract: enhancedInstruction, max_tokens: tokensPerUrl },
            llmProcessor
          );

          if (llmResult.processed) {
            postContent = `## LLM Analysis: ${result.post.title}\n\n**r/${result.post.subreddit}** ‚Ä¢ u/${result.post.author} ‚Ä¢ ‚¨ÜÔ∏è ${result.post.score} ‚Ä¢ üí¨ ${result.post.commentCount} comments\nüîó ${result.post.url}\n\n${llmResult.content}`;
            mcpLog('debug', `[${successful}/${urls.length}] LLM extraction complete`, 'reddit');
          } else {
            llmErrors++;
            mcpLog('warning', `[${successful}/${urls.length}] LLM extraction failed: ${llmResult.error || 'unknown'}`, 'reddit');
          }
        }

        contents.push(postContent);
      }
    }

    // Build 70/20/10 response
    const batchHeader = formatBatchHeader({
      title: `Reddit Posts`,
      totalItems: urls.length,
      successful,
      failed,
      ...(fetchComments ? { extras: { 'Comments/post': commentsPerPost } } : {}),
      ...(use_llm ? { tokensPerItem: tokensPerUrl } : {}),
      batches: totalBatches,
    });

    const statusExtras: string[] = [];
    if (batchResult.rateLimitHits > 0) {
      statusExtras.push(`‚ö†Ô∏è ${batchResult.rateLimitHits} rate limit retries`);
    }
    if (use_llm && !llmProcessor) {
      statusExtras.push('‚ö†Ô∏è LLM unavailable (OPENROUTER_API_KEY not set)');
    } else if (llmErrors > 0) {
      statusExtras.push(`‚ö†Ô∏è ${llmErrors} LLM extraction failures`);
    }

    const nextSteps = [
      successful > 0 ? 'VERIFY CLAIMS: web_search(keywords=["topic claim1 verify", "topic claim2 official docs", "topic best practices"]) ‚Äî community says X, verify with web' : null,
      successful > 0 ? 'SCRAPE REFERENCED LINKS: scrape_links(urls=[...URLs found in comments...], use_llm=true, what_to_extract="Extract evidence | data | recommendations") ‚Äî follow external links from discussions' : null,
      'BROADEN: search_reddit(queries=[...related angles...]) ‚Äî if more perspectives needed',
      successful > 0 ? 'SYNTHESIZE (only after verifying + scraping): deep_research(questions=[{question: "Based on verified Reddit findings about [topic]..."}])' : null,
      failed > 0 ? 'Retry failed URLs individually' : null,
    ].filter(Boolean) as string[];

    const extraStatus = statusExtras.length > 0 ? `\n${statusExtras.join(' | ')}` : '';

    return formatSuccess({
      title: `Reddit Posts Fetched (${successful}/${urls.length})`,
      summary: batchHeader + extraStatus,
      data: contents.join('\n\n---\n\n'),
      nextSteps,
    });
  } catch (error) {
    const structuredError = classifyError(error);
    return formatError({
      code: structuredError.code,
      message: structuredError.message,
      retryable: structuredError.retryable,
      toolName: 'get_reddit_post',
      howToFix: ['Verify REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET are set'],
      alternatives: [
        'web_search(keywords=["topic reddit discussion", "topic reddit recommendations"]) ‚Äî search for Reddit content via web search instead',
        'scrape_links(urls=[...the Reddit URLs...], use_llm=true, what_to_extract="Extract post content | top comments | recommendations") ‚Äî scrape Reddit pages directly as a fallback',
        'deep_research(questions=[{question: "What are community opinions on [topic]?"}]) ‚Äî get AI-synthesized community perspective',
      ],
    });
  }
}
